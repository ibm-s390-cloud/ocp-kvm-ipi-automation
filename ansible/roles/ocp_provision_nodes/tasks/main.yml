---

- name: run soundness checks for cluster node provisioning
  when: disable_soundness_check is undefined
  ansible.builtin.include_tasks: '{{ role_path }}/tasks/soundness_checks.yml'

- name: determine name of existing cluster from installation log
  ansible.builtin.include_tasks: '{{ inventory_dir }}/tasks/get_cluster_name.yml'

- name: generate list of all existing cluster nodes (pre-scaling)
  block:
    - name: fetch list of all existing cluster nodes
      ansible.builtin.include_tasks: '{{ inventory_dir }}/tasks/get_cluster_nodes.yml'

    - name: retain list of existing cluster worker nodes for future reference
      ansible.builtin.set_fact:
        worker_domains_pre_scale: '{{ worker_domains }}'

- name: provision additional worker nodes
  block:
    - name: scale up worker node replicas by modifying the corresponding MachineSet resource
      vars:
        spec_replicas_patch: |-
          spec:
            replicas: {{ (worker_domains_pre_scale | length) + addl_cluster_nodes | int }}
      kubernetes.core.k8s:
        kubeconfig: '{{ openshift_installer_workdir }}/auth/kubeconfig'
        api_version: 'machine.openshift.io/v1beta1'
        kind: MachineSet
        name: '{{ cluster_id }}-worker-0'
        namespace: openshift-machine-api
        state: present
        definition: '{{ spec_replicas_patch | from_yaml }}'

    - name: wait for the additional worker nodes to be available
      kubernetes.core.k8s_info:
        kubeconfig: '{{ openshift_installer_workdir }}/auth/kubeconfig'
        api_version: 'machine.openshift.io/v1beta1'
        kind: MachineSet
        name: '{{ cluster_id }}-worker-0'
        namespace: openshift-machine-api
      register: worker_machineset
      until: worker_machineset.resources[0].status.readyReplicas == ((worker_domains_pre_scale | length) + addl_cluster_nodes | int)
      retries: 20
      delay: 30

- name: generate list of all existing cluster nodes (post-scaling)
  block:
    - name: fetch list of all existing cluster nodes
      ansible.builtin.include_tasks: '{{ inventory_dir }}/tasks/get_cluster_nodes.yml'

    - name: generate list of names of added worker nodes
      ansible.builtin.set_fact:
        added_worker_domains: '{{ worker_domains | difference(worker_domains_pre_scale) }}'

- name: configuration handling specific to infrastructure nodes
  when: "cluster_node_type == 'infra'"
  block:
    - name: label existing worker nodes as dedicated application nodes
      vars:
        node_label_patch: |-
          metadata:
            labels:
              node-role.kubernetes.io/app: ""
      kubernetes.core.k8s:
        kubeconfig: '{{ openshift_installer_workdir }}/auth/kubeconfig'
        api_version: v1
        kind: Node
        name: '{{ item }}'
        state: present
        definition: '{{ node_label_patch | from_yaml }}'
      loop: '{{ worker_domains_pre_scale }}'

    - name: mark additional worker nodes as dedicated infrastructure nodes
      kubernetes.core.k8s:
        kubeconfig: '{{ openshift_installer_workdir }}/auth/kubeconfig'
        state: present
        definition: '{{ lookup("template", "{{ role_path }}/templates/infra-node.yaml.j2") | from_yaml }}'
      loop: '{{ added_worker_domains }}'

    - name: reschedule corresponding cluster workloads to dedicated infrastructure nodes
      kubernetes.core.k8s:
        kubeconfig: '{{ openshift_installer_workdir }}/auth/kubeconfig'
        state: present
        definition: '{{ item }}'
      loop:
        - '{{ lookup("file", "{{ role_path }}/files/ingresscontroller-default.yaml") | from_yaml }}'
        - '{{ lookup("file", "{{ role_path }}/files/imageregistry-cluster.yaml") | from_yaml }}'
        - '{{ lookup("file", "{{ role_path }}/files/configmap-clustermonitoringconfig.yaml") | from_yaml }}'

    - name: wait for the cluster to apply changes
      ansible.builtin.wait_for:
        timeout: 300
      delegate_to: localhost

    - name: reconfigure haproxy to account for added infrastructure nodes
      block:
        - name: fetch list of all existing cluster infrastructure nodes
          ansible.builtin.include_tasks: '{{ inventory_dir }}/tasks/get_infra_nodes.yml'

        - name: retrieve XML of libvirt cluster network
          community.libvirt.virt_net:
            command: get_xml
            name: '{{ cluster_id }}'
          register: libvirt_cluster_network

        - name: determine network configuration for cluster nodes
          community.general.xml:
            xmlstring: '{{ libvirt_cluster_network.get_xml }}'
            xpath: /network/ip/dhcp/host
            content: attribute
          register: libvirt_cluster_nodes

        - name: update haproxy configuration
          vars:
            nodes_query: "matches[?contains({{ infra_domains | ansible.builtin.to_json | regex_replace('\"', \"'\") }}, host.name)] || [`0`]"
            infra_nodes_details: '{{ libvirt_cluster_nodes | to_json | from_json | community.general.json_query(nodes_query) }}'
          ansible.builtin.include_role:
            name: networking
            tasks_from: configure_haproxy

- name: configuration handling specific to worker nodes
  when: "cluster_node_type == 'worker'"
  block:
    - name: fetch list of all existing cluster infrastructure nodes
      ansible.builtin.include_tasks: '{{ inventory_dir }}/tasks/get_infra_nodes.yml'

    - name: reconfigure haproxy to account for added worker nodes
      when: infra_domains | length > 0
      block:
        - name: retrieve XML of libvirt cluster network
          community.libvirt.virt_net:
            command: get_xml
            name: '{{ cluster_id }}'
          register: libvirt_cluster_network

        - name: determine network configuration for cluster nodes
          community.general.xml:
            xmlstring: '{{ libvirt_cluster_network.get_xml }}'
            xpath: /network/ip/dhcp/host
            content: attribute
          register: libvirt_cluster_nodes

        - name: update haproxy configuration
          vars:
            nodes_query: "matches[?contains({{ infra_domains | ansible.builtin.to_json | regex_replace('\"', \"'\") }}, host.name)] || [`0`]"
            infra_nodes_details: '{{ libvirt_cluster_nodes | to_json | from_json | community.general.json_query(nodes_query) }}'
          ansible.builtin.include_role:
            name: networking
            tasks_from: configure_haproxy

    - name: update haproxy configuration
      when: infra_domains | length == 0
      vars:
        cluster_number_of_workers: '{{ worker_domains | length }}'
      ansible.builtin.include_role:
        name: networking
        tasks_from: configure_haproxy

- name: configuration handling common to worker and infrastructure nodes
  block:
    - name: make sure the haproxy service is started
      ansible.builtin.service:
        name: haproxy
        state: started
        enabled: true

    - name: update existing vbmc / ipmi configuration to include additional cluster nodes (if applicable)
      vars:
        vbmc_ipmi: '{{ setup_vbmc_ipmi | default(true) }}'
      when: vbmc_ipmi
      ansible.builtin.include_role:
        name: vbmc

    - name: update existing SSH configuration for easy access to cluster nodes
      ansible.builtin.include_role:
        name: ocp_install_cluster_wrapup
        tasks_from: persist_ssh_config
